<div class="step-text">
<p></p><p>Our computers do not have the inherent capability to understand unmediated human language. They need to incorporate special encoding systems to convert it to the machine language first. <strong>Unicode</strong> is one of these character encoding standards that help our computers comprehend the text we input and process it across different environments without a hitch. Just like the majority of today's programming languages, JavaScript supports the Unicode character set and provides functions or libraries for manipulating Unicode characters.</p>
<h5 id="unicode">Unicode</h5>
<p>In machine language, characters are represented using sequences of 0s and 1s. The<strong> ASCII (American Standard Code for Information Interchange)</strong>, which is one of the earliest and most widely used encoding scheme, uses a combination of 7 digits, either 0 or 1, to represent a character. This binary sequence allows for a total of 128 different characters to be represented. Due to its limited coverage in terms of sequence combinations, ASCII primarily includes a limited set of characters, such as basic Latin letters, numbers, and a few symbols.</p>
<p>As a more refined alternative to ASCII, Unicode is a universal character encoding standard also used by most of the modern programming languages today. It's a character set that includes universally representation of far more diverse characters and symbols, including those from scripts like Latin, Cyrillic, Chinese, Arabic, and even emojis! ü§© </p>
<h5 id="code-points">Code points</h5>
<p><strong>UTF-8 (8-Bit Unicode Transformation Format)</strong> and <strong>UTF-16 (16-Bit Unicode Transformation Format)</strong> are encoding schemes that define rules for representing Unicode characters in binary form. These encoding schemes assign unique addresses to the characters. Those address values, known as <strong>code points</strong>, enable precise identification of the characters within the Unicode character set and can be represented either as hexadecimal values (which is a number system using 16 symbols: 0 to 9 and A to F) preceded by the <code class="language-javascript">U+</code> prefix or simply as decimal numbers. For example, the letter "A" has the code point value of <code class="language-javascript">U+0041</code> or <code class="language-javascript">65</code>, and the heart symbol "‚ù§Ô∏è" is assigned to the code point <code class="language-javascript">U+2764</code> or <code class="language-javascript">10084</code> in the Unicode chart.</p>
<p>The key difference between UTF-8 and UTF-16 is that UTF-8 uses a minimum of 8 bits (1 byte) for a character, and UTF-16 uses a minimum of 16 bits (2 bytes). However, there are also some characters in Unicode that can't fit within a single 16-bit unit. These characters have code points beyond the <strong>BMP (Basic Multilingual Plane)</strong>, which cover commonly used characters within Unicode, and they are located in a special range known as the<strong> Supplementary Multilingual Plane (SMP).</strong> The SMP includes code points ranging from U+10000 to U+1FFFF. To represent code points, the characters within this range, UTF-16 encoding utilize a special method referred to as surrogate pairs.</p>
<h5 id="surrogate-pairs">Surrogate pairs</h5>
<p><strong>Surrogate pairs</strong> are basically dual combinations of special 16-bit values. By using surrogate pairs, UTF-16 can represent characters that require more than 16 bits. The high and low surrogate values work together to encode and represent these extended characters accurately. To exemplify, the emoji "üêá" has the surrogate value of <code class="language-javascript">U+D83D U+DC07</code> in UTF-16, or <code class="language-javascript">55356 56455</code> in decimal representation.</p>
<p>If you look at the code points mentioned earlier for the letter "A" and the heart symbol, you will see that they don't need surrogate pairs because their code point is smaller than <code class="language-javascript">U+FFFF</code>. On the other hand, the heart symbol has a code point greater than <code class="language-javascript">U+FFFF</code>, so it does require surrogate pairs.</p>
<h5 id="unicode-functions-in-javascript">Unicode functions in JavaScript</h5>
<p>JavaScript includes multiple functions and methods specifically designed to work with Unicode characters, and they can be applied to various data types, including strings, arrays, and objects. These functions enable interaction with code points, converting them to characters and facilitating operations on Unicode-encoded strings. Let's dive in to understand their inner workings!</p>
<ul>
<li><strong>codePointAt()</strong></li>
</ul>
<p><code class="language-javascript">codePointAt()</code> is a method that allows us to work with Unicode. To use <code class="language-javascript">codePointAt()</code>, we need to have a string and specify the index of the character. It will return Unicode code point of the character at the given index.</p>
<p>In the following example, we have a string called <code class="language-javascript">text</code>. To identify the character at index 11 in this string, which is the lowercase letter "w" in this case, we first use <code class="language-javascript">console.log(text[11])</code>. Then, to find the Unicode code point of the character "w", we use '11' as a parameter inside the <code class="language-javascript">codePointAt()</code> method and print the value to display the result on the console.</p>
<pre><code class="language-javascript">let text = "Follow the white rabbit.";
let code = text.codePointAt(11);
console.log(text[11]); // "w"
console.log(code); // 119</code></pre>
<ul>
<li><strong>charCodeAt():</strong></li>
</ul>
<p>Similar to the <code class="language-javascript">codePointAt()</code> method, <code class="language-javascript">charCodeAt()</code> also takes the index of the character as a parameter within the string. However, the key distinction that sets it apart from <code class="language-javascript">charCodeAt()</code> is that it operates with UTF-16 encoding along with Unicode. So, what does that suggest?</p>
<p>Here's the answer: the UTF-16 code unit is a 16-bit value within the range of 0 to 65535, in decimals. So the applicability of the <code class="language-javascript">thecharCodeAt()</code> method is limited to characters within the <strong>Basic Multilingual Plane (BMP)</strong>. On the other hand, Unicode is a character set that encompasses a much larger range of characters, including BMP. Although both <code class="language-javascript">codePointAt()</code> and <code class="language-javascript">charCodeAt()</code> return integers representing character codes, <code class="language-javascript">charCodeAt()</code> is constrained to the UTF-16 range of 0 to 65535 whereas <code class="language-javascript">codePointAt()</code> is capable of handling the entire range of Unicode values.</p>
<pre><code class="language-javascript">let text = "Follow the white rabbit.";
let code = text.charCodeAt(11);
console.log(text[11]); // "w"
console.log(code); // 119</code></pre>
<p>When we look at the example, we can see that the only updated part is the function we use, but the output remains the same. The reason behind this is that the lowercase "w" character is part of BMP, which means it is commonly used. So, both the Unicode code point and UTF-16 code units for this character will be the same.</p>
<p>Let's consider the situation where our character lies outside the Basic Multilingual Plane (BMP). What would happen then?</p>
<pre><code class="language-javascript">let text = "Follow the üêá.";
let code1 = text.codePointAt(11);
let code2 = text.charCodeAt(11);
console.log(code1); // 128007
console.log(code2); // 55357</code></pre>
<p>At this point, surrogate pairs come into play! In this example, the character "üêá" is represented by a Unicode code point <code class="language-javascript">U+1F407</code>. As you remember, this character does have a surrogate value in UTF-16, "üêá" is <code class="language-javascript">U+D83D U+DC07</code> in hexadecimal representation, and it is beyond the BMP, it requires more than one UTF-16 code unit to represent it. That's why, when you use <code class="language-javascript">codePointAt(11)</code> method, it correctly returns the full Unicode code point of "üêá", which is <code class="language-javascript">128007</code> in decimal. On the other hand, when you use <code class="language-javascript">charCodeAt(11)</code> method, it only returns the UTF-16 code unit of the first surrogate of "üêá", which is <code class="language-javascript">55357</code> in decimal. This is because <code class="language-javascript">charCodeAt()</code> operates with UTF-16 code units and doesn't handle surrogate pairs as a single entity.</p>
<ul>
<li><strong>String.fromCodePoint() and String.fromCharCode()</strong> </li>
</ul>
<p>Lastly, we use <code class="language-javascript">String.fromCharCode()</code> and <code class="language-javascript">String.fromCodePoint</code> methods in JavaScript to convert Unicode code points into strings. Again, the main difference between <code class="language-javascript">String.fromCodePoint()</code> and <code class="language-javascript">String.fromCharCode()</code> in JavaScript lies in the range of characters they can handle and the way they accept input: the former works with Unicode, while the latter operates with UTF-16. These methods are useful when we need to work with characters that are represented by their numerical codes. </p>
<pre><code class="language-javascript">const text1 = String.fromCodePoint(84, 72, 69, 32, 77, 65, 84, 82, 73, 88);
console.log(text1); // "THE MATRIX"
const text2 = String.fromCharCode(84, 72, 69, 32, 77, 65, 84, 82, 73, 88);
console.log(text2); // "THE MATRIX"</code></pre>
<p>In this example, these code unit values correspond to characters within the BMP, the resulting string will be the same, regardless of whether you use <code class="language-javascript">String.fromCharCode()</code> or <code class="language-javascript">String.fromCodePoint()</code>.</p>
<h5 id="conclusion">Conclusion</h5>
<p>Our computers rely on encoding systems to comprehend human language. Unicode is a universal standard for encoding and representation of wide-ranging characters and symbols. UTF-8 and UTF-16 are encoding systems within this standard, and they define rules for representing characters in binary form. Surrogate pairs, on the other hand, are pairs of Unicode values within the UTF-16 system, and they allow for the representation of characters exceeding 16 bits. JavaScript programming language offers functions like <code class="language-javascript">codePointAt()</code> and <code class="language-javascript">charCodeAt()</code> to handle Unicode and UTF-16 characters. Finally, <code class="language-javascript">String.fromCodePoint()</code> and <code class="language-javascript">String.fromCharCode()</code> convert Unicode code points into strings. These methods help computers work with and convert characters in different forms within the JavaScript context. </p>
</div>